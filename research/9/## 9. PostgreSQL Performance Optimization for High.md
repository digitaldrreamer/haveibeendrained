<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# \#\# 9. PostgreSQL Performance Optimization for High-Frequency Queries

### Research Questions

**Indexing Strategies:**

- What types of indexes are available in PostgreSQL (B-tree, Hash, GIN, GiST, etc.)?
- When should you use each index type?
- How do you create partial indexes for frequently accessed subsets?
- What are the trade-offs of indexing (query speed vs write speed vs storage)?

**JSONB Optimization:**

- How do you efficiently query JSONB columns?
- What indexes work with JSONB (GIN, expression indexes)?
- Should you use JSONB for storing analysis results, or normalize the data?

**Connection Pooling:**

- What connection pooling libraries are available for Node.js (pg-pool, pgbouncer)?
- What are the recommended pool size settings?
- How do you handle connection pool exhaustion?

**Query Optimization:**

- How do you use `EXPLAIN ANALYZE` to identify slow queries?
- What are common query anti-patterns to avoid?
- How do you optimize JOIN queries?

**Maintenance:**

- What are autovacuum settings, and when should you tune them?
- How do you monitor table bloat?
- What are best practices for regular maintenance (VACUUM, ANALYZE, REINDEX)?

**Scaling:**

- When should you consider read replicas?
- How do you implement read replicas with PostgreSQL?
- What are the options for horizontal scaling (sharding, Citus, etc.)?


### Desired Outputs

- Index strategy for the specific schema (drainers, analysis_cache, reports_pending)
- Connection pooling configuration example
- Query optimization checklist
- Monitoring queries for identifying performance issues
- Scaling roadmap for growing from 100 to 10,000+ daily users

---

Excellent! Now let me create one final summary document with quick-start implementation guide:

![PostgreSQL Performance Optimization Decision Tree for Have I Been Drained](https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/60f2fa59c5effa2b31e19e18058162d5/0e1bc94e-64da-42a9-ae1f-0e8809fac236/04220b8c.png)

PostgreSQL Performance Optimization Decision Tree for Have I Been Drained

# PostgreSQL Performance Optimization for Have I Been Drained

## Executive Summary

This comprehensive guide provides production-ready PostgreSQL optimization strategies for the **Have I Been Drained** Solana wallet security checker project. The research covers indexing strategies, JSONB optimization, connection pooling, query optimization, maintenance, and scaling roadmaps specifically tailored to your project's needs.[^1][^2][^3]

## Current State of Technology (December 2025)

PostgreSQL 16-17 offers significant performance improvements with native partitioning, better JSONB handling, and improved parallel query execution. Modern approaches emphasize **proactive monitoring** over reactive optimization, with tools like `pg_stat_statements` and pganalyze providing real-time performance visibility.[^1][^4][^5][^6][^7][^8]

### Key 2025 Trends

- **GIN indexes for JSONB** are now the standard for semi-structured data, with `jsonb_path_ops` offering 10x+ space savings for containment queries[^2][^9][^10]
- **Connection pooling** via PgBouncer or integrated solutions (Neon, Supabase) eliminates the overhead of creating new database connections[^3][^11]
- **Read replicas with streaming replication** enable zero-downtime scaling for read-heavy workloads[^12][^13]
- **Citus extension** for distributed PostgreSQL is production-ready for SaaS and analytical workloads[^14][^15]

***

## 1. Indexing Strategies

### Index Types \& When to Use Each

PostgreSQL provides six main index types, each optimized for specific query patterns:[^1][^4][^5]


| **Index Type** | **Best For** | **Key Trade-off** | **Example for Have I Been Drained** |
| :-- | :-- | :-- | :-- |
| **B-tree** (default) | Equality, ranges, sorting | Largest size | `CREATE INDEX idx_drainers_wallet ON drainers(wallet_address)` |
| **Hash** | Exact matches only | Cannot handle ranges | `CREATE INDEX idx_signature USING HASH ON drainers(signature_hash)` |
| **GIN** | JSONB, arrays, full-text | Slower writes | `CREATE INDEX idx_results USING GIN ON analysis_cache(analysis_results)` |
| **GiST** | Spatial data, complex types | Slower than B-tree | Less common for wallet analysis |
| **BRIN** | Time-series (1GB+ tables) | Lower precision | `CREATE INDEX idx_timestamp USING BRIN ON drainers(created_at)` |
| **SP-GiST** | Hierarchical/sparse data | Specialized | IP address ranges for threat intel |

### Implementation for Have I Been Drained Schema

#### Drainers Table Indexing Strategy

```sql
-- PRIMARY LOOKUP (Most frequent operation)
CREATE INDEX idx_drainers_wallet_active 
ON drainers(wallet_address) 
WHERE status = 'active';  -- Partial index (6-10x space savings)

-- COMPOSITE INDEX for dashboard queries
CREATE INDEX idx_drainers_status_type_date 
ON drainers(status, attack_type, created_at DESC)
WHERE status = 'active';

-- TIME-SERIES INDEX (BRIN for efficient scanning)
CREATE INDEX idx_drainers_activity_brin 
ON drainers USING BRIN (last_activity_timestamp)
WHERE status = 'active';

-- SIGNATURE MATCHING (for pattern detection)
CREATE INDEX idx_drainers_signature_hash 
ON drainers(signature_hash);
```

**Key insight from research:**  Partial indexes reduce index size and write overhead by focusing on frequently queried subsets. For "Have I Been Drained," since most queries filter on `status = 'active'`, a partial index saves ~90% storage compared to full indexing.[^16][^17]

#### Analysis Cache Table Optimization

```sql
-- GIN INDEX for complex JSONB containment queries
CREATE INDEX idx_analysis_results_gin 
ON analysis_cache USING GIN (analysis_results)
WHERE is_valid = true AND cache_hit = true;

-- EXPRESSION INDEX for frequently extracted JSON fields
CREATE INDEX idx_analysis_threat_level 
ON analysis_cache ((analysis_results->>'threat_level'))
WHERE (analysis_results->>'threat_level') IN ('high', 'critical');

-- PARTIAL INDEX for cache freshness
CREATE INDEX idx_analysis_recent 
ON analysis_cache(wallet_address, analysis_timestamp)
WHERE analysis_timestamp > NOW() - INTERVAL '7 days'
  AND is_valid = true;
```

**Performance improvement:**  GIN indexes with `jsonb_path_ops` reduce query time from seconds to milliseconds for containment checks like `WHERE analysis_results @> '{"threat_level": "critical"}'::jsonb`[^2][^10]

### Index Maintenance \& Bloat Management

Index bloat accumulates over time as rows are deleted/updated, causing queries to read unnecessary data:[^18][^19][^20][^21]

```sql
-- Detect bloated indexes (> 15% waste)
SELECT 
  indexname,
  pg_size_pretty(pg_relation_size(indexrelid)) as size,
  ROUND(100.0 * (pg_relation_size(indexrelid) - pg_relation_size(indexrelid, 'main')) / 
        pg_relation_size(indexrelid), 2) as bloat_pct
FROM pg_stat_user_indexes
WHERE bloat_pct > 15;

-- REINDEX CONCURRENTLY (non-blocking)
REINDEX INDEX CONCURRENTLY idx_drainers_wallet_active;
```


***

## 2. JSONB Optimization

### JSONB vs. Normalization Decision

The decision between storing data in JSONB vs. normalized columns depends on query patterns:[^9][^10]

**Use JSONB for:**

- Semi-structured data with varying schemas (analysis results vary by wallet type)
- Data that changes infrequently
- Metadata that doesn't need frequent filtering

**Normalize to columns if:**

- Column appears in >30% of queries (threat_level is queried by most reports)
- Need consistent, type-safe filtering
- Want maximum performance (columns are 2-3x faster than JSONB extraction)


### Optimal Schema for Have I Been Drained

```sql
CREATE TABLE analysis_cache (
  id BIGSERIAL PRIMARY KEY,
  wallet_address TEXT NOT NULL,
  
  -- Frequently queried: normalize these from JSONB
  threat_level TEXT CHECK (threat_level IN ('low', 'medium', 'high', 'critical')),
  is_valid BOOLEAN NOT NULL DEFAULT true,
  
  -- Complex/variable: keep in JSONB
  analysis_results JSONB NOT NULL,  -- Contains: detected_drains[], attack_patterns[], recommendations[]
  
  analysis_timestamp TIMESTAMP NOT NULL,
  expires_at TIMESTAMP NOT NULL,
  accessed_at TIMESTAMP,
  
  created_at TIMESTAMP DEFAULT NOW(),
  
  -- Indexes
  CONSTRAINT expires_before_accessed CHECK (expires_at > analysis_timestamp)
);
```


### JSONB Query Patterns \& Index Strategy

Not all JSONB queries benefit from GIN indexes. The table below shows which queries use indexes:[^2][^10]


| **Query Type** | **Example** | **Index Type** | **Speed Improvement** |
| :-- | :-- | :-- | :-- |
| **Containment** | `analysis_results @> '{...}'` | GIN | 10-100x |
| **Key existence** | `analysis_results ? 'key'` | GIN | 5-50x |
| **Path extraction** | `analysis_results->>'field'` | Expression | 2-5x |
| **Array contains** | `analysis_results['array'] ?& ['a','b']` | GIN | 10-100x |

**Example implementation:**

```sql
-- Query 1: Find critical threat analyses (uses both column and GIN)
SELECT wallet_address, analysis_results->'detected_drains'
FROM analysis_cache
WHERE threat_level = 'critical'  -- Column index (fastest)
  AND analysis_results @> '{"expired": false}'::jsonb  -- GIN index
ORDER BY accessed_at DESC LIMIT 100;

-- Indexes:
CREATE INDEX idx_analysis_threat_col ON analysis_cache(threat_level);
CREATE INDEX idx_analysis_expired_gin ON analysis_cache USING GIN (analysis_results)
WHERE threat_level IN ('high', 'critical');

-- Query 2: Extract specific drainer from nested JSONB
SELECT (jsonb_array_elements(analysis_results->'detected_drains')->>'drainer_address') as drainer
FROM analysis_cache
WHERE wallet_address = $1;

-- Better approach: normalize high-access patterns
CREATE TABLE analysis_detected_drains (
  id BIGSERIAL PRIMARY KEY,
  analysis_cache_id BIGINT NOT NULL,
  drainer_address TEXT NOT NULL,
  confidence NUMERIC(3,2),
  transaction_count INT,
  FOREIGN KEY (analysis_cache_id) REFERENCES analysis_cache(id)
);
CREATE INDEX idx_drains_drainer ON analysis_detected_drains(drainer_address);
```

**Key insight:**  For wallet analysis, normalizing `detected_drains` into a separate table eliminates the need to extract JSON arrays repeatedly, improving query speed by 3-5x while enabling proper indexing.[^10]

***

## 3. Connection Pooling

### Why Connection Pooling Matters

PostgreSQL can only handle one query per connection serially. Without pooling, each request would need to establish a TCP connection (~200ms overhead) and authenticate (~50ms), making the database appear slow even if queries are fast.[^22][^23][^24]

**Impact:** Without pooling, 100 concurrent requests = 100+ database connections, consuming 1GB+ RAM. With pooling, 20 connections serve all requests, using ~200MB RAM.

### Node.js pg-pool Configuration for Have I Been Drained

The `pg` package (used with Hono/Node.js) includes built-in pooling via `pg-pool`:[^23][^24]

```javascript
// Optimal configuration
const pool = new Pool({
  // Pool sizing: (CPU cores * 2) + spare connections
  // Development (4 cores): 8 + 5 = 13
  // Production (8 cores): 16 + 5 = 21
  max: parseInt(process.env.DB_POOL_SIZE || '20'),
  min: parseInt(process.env.DB_POOL_MIN || '5'),
  
  // Timeouts to prevent hanging
  idleTimeoutMillis: 30000,      // 30s idle = release connection
  connectionTimeoutMillis: 5000,  // 5s to acquire connection
  
  // Query timeout (prevent runaway queries)
  statement_timeout: 30000,  // 30 seconds per query
  
  // Connection identification for monitoring
  application_name: 'have-i-been-drained',
});
```

**Resource calculation:**

```
Max connections needed = concurrent_users * avg_queries_per_request
Typical web app: 100 users * 1.5 queries = 150 needed
Pool size = 20 (reuses connections via queueing)
Saturation = 150 / 20 = 7.5x (queue depth, acceptable)
```


### Pool Exhaustion Handling[^3]

When the pool runs out of available connections, requests queue. If queue exceeds timeout, requests fail:

```javascript
// Circuit breaker pattern for graceful degradation
class DatabaseCircuitBreaker {
  async execute(queryFn) {
    if (this.state === 'OPEN') {
      throw new Error('Database circuit breaker is OPEN');
    }
    try {
      return await queryFn();
    } catch (error) {
      this.failureCount++;
      if (this.failureCount > 5) this.state = 'OPEN';
      throw error;
    }
  }
}

// Retry with exponential backoff
async function executeWithRetry(queryFn, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await queryFn();
    } catch (error) {
      if (i < maxRetries - 1) {
        const delay = Math.pow(2, i) * 100;  // 100ms, 200ms, 400ms
        await new Promise(r => setTimeout(r, delay));
      } else throw error;
    }
  }
}
```


### Free Connection Pooling Solutions

For the "Have I Been Drained" hackathon, these services provide connection pooling for free:[^3][^11]

1. **Neon** (recommended for Postgres)[^3]
    - Free tier: Connection pooling included
    - PgBouncer built-in
    - Cost after free: ~\$0.10/hour for small projects
2. **Supabase** (best for on-chain integration)
    - Built-in connection pooling
    - Real-time subscriptions for wallet events
    - Free: 500MB database + 1GB file storage
3. **Railway** (simplest deployment)
    - Free: \$5 credit/month
    - PostgreSQL: ~\$0.30/hour
    - No connection pooling, but low-latency infrastructure

***

## 4. Query Optimization

### Using EXPLAIN ANALYZE

The `EXPLAIN ANALYZE` command shows the actual execution plan and timing:[^18][^19][^25]

```sql
-- Comprehensive analysis with cache info
EXPLAIN (ANALYZE, BUFFERS, VERBOSE, TIMING)
SELECT d.wallet_address, COUNT(r.id) as report_count
FROM drainers d
LEFT JOIN reports_pending r ON d.id = r.drainer_id
WHERE d.status = 'active'
GROUP BY d.wallet_address
ORDER BY report_count DESC
LIMIT 50;

-- Interpretation:
-- "Seq Scan" = full table read (bad for large tables)
-- "Index Scan" = using an index (good)
-- "Filter rows >> returned rows" = inefficient WHERE clause
-- "Shared Buffers Hit" > 95% = data in cache (good)
```

**When to optimize:**

- Execution time > 100ms for single query
- More than 1,000 rows read per returned row
- Sequential scans on tables with >100K rows


### Common Query Anti-Patterns to Avoid[^26][^27][^28][^29][^18]

#### Anti-Pattern 1: SELECT * (Prevents Index-Only Scans)

```sql
-- ❌ BAD: Returns all columns, prevents performance optimization
SELECT * FROM drainers WHERE status = 'active';

-- ✅ GOOD: Return only needed columns
SELECT id, wallet_address, threat_level FROM drainers WHERE status = 'active';
```


#### Anti-Pattern 2: N+1 Query Problem[^28][^29][^30]

```javascript
// ❌ BAD: 1 query for drainers + N queries for reports each
const drainers = await db.query('SELECT * FROM drainers WHERE status = $1', ['active']);
for (const drainer of drainers) {
  const reports = await db.query('SELECT * FROM reports WHERE drainer_id = $1', [drainer.id]);
  drainer.reports = reports;  // Loop causes N additional queries!
}

// ✅ GOOD: Single JOIN query (2 queries total)
const result = await db.query(`
  SELECT d.*, array_agg(r.*) as reports
  FROM drainers d
  LEFT JOIN reports r ON d.id = r.drainer_id
  WHERE d.status = $1
  GROUP BY d.id
`, ['active']);
```

**Performance impact:** N+1 queries increase latency from 50ms to 500ms+ for 100 items.[^29]

#### Anti-Pattern 3: Functions in WHERE Clause (Prevents Index Use)[^28]

```sql
-- ❌ BAD: Function prevents index on wallet_address
SELECT * FROM drainers WHERE LOWER(wallet_address) = LOWER($1);

-- ✅ GOOD: Create expression index
CREATE INDEX idx_drainers_wallet_lower ON drainers(LOWER(wallet_address));
SELECT * FROM drainers WHERE LOWER(wallet_address) = LOWER($1);  -- Now uses index
```


#### Anti-Pattern 4: OR Without UNION[^27]

```sql
-- ❌ BAD: Cannot use multiple indexes efficiently
SELECT * FROM drainers WHERE threat_level = 'critical' OR status = 'flagged';

-- ✅ GOOD: UNION uses separate indexes
SELECT * FROM drainers WHERE threat_level = 'critical'
UNION
SELECT * FROM drainers WHERE status = 'flagged';
```


### JOIN Optimization[^26][^31]

Three JOIN strategies exist; PostgreSQL chooses automatically but indexes affect the choice:


| **Join Type** | **When Used** | **Index to Help** |
| :-- | :-- | :-- |
| **Nested Loop** | Small outer table | Index on inner table's join key |
| **Hash Join** | Both tables medium-large | None (uses hash table in RAM) |
| **Merge Join** | Both tables sorted | Indexes on both join keys |

```sql
-- Optimize large JOIN operations
EXPLAIN ANALYZE
SELECT d.wallet_address, array_agg(DISTINCT r.attack_type) as attack_types
FROM drainers d
INNER JOIN reports_pending r ON d.id = r.drainer_id
WHERE d.status = 'active' AND r.status = 'pending'
GROUP BY d.id, d.wallet_address
HAVING COUNT(r.id) > 5;

-- Supporting indexes (speeds up all three join types)
CREATE INDEX idx_reports_drainer_id ON reports_pending(drainer_id);
CREATE INDEX idx_drainers_id_status ON drainers(id, status);
```


***

## 5. Maintenance

### Autovacuum Tuning[^32][^33][^20][^21]

PostgreSQL uses MVCC (Multi-Version Concurrency Control), which creates "dead tuples" (old row versions) when rows are updated/deleted. Autovacuum automatically cleans these up, but default settings are conservative to avoid interfering with workloads.[^32][^33]

**For Have I Been Drained's high-write tables:**

```sql
-- HIGH-WRITE: reports_pending (frequent updates/deletes)
ALTER TABLE reports_pending SET (
  autovacuum_vacuum_threshold = 1000,      -- Trigger at 1000 changes (vs 50 default)
  autovacuum_vacuum_scale_factor = 0.05,   -- OR 5% of table size (vs 20% default)
  autovacuum_vacuum_cost_delay = 5,        -- Be more aggressive
  autovacuum_vacuum_cost_limit = 10000     -- Increase cost limit
);

-- MODERATE-WRITE: drainers (occasional updates)
ALTER TABLE drainers SET (
  autovacuum_vacuum_scale_factor = 0.1,    -- 10%
  autovacuum_analyze_scale_factor = 0.05   -- Refresh stats more often
);

-- Enable logging to monitor effectiveness
ALTER SYSTEM SET log_autovacuum_min_duration = 0;
SELECT pg_reload_conf();
```

**Monitoring autovacuum effectiveness:**

Table bloat (dead tuples) degrades performance. A table with >10% dead tuples is considered bloated:[^20][^34]

```sql
SELECT 
  schemaname, tablename,
  n_live_tup, n_dead_tup,
  ROUND(100.0 * n_dead_tup / (n_live_tup + n_dead_tup), 2) as dead_ratio_pct,
  last_autovacuum
FROM pg_stat_user_tables
WHERE dead_ratio_pct > 10
ORDER BY n_dead_tup DESC;
```


### Bloat Remediation[^34][^21][^20]

| **Bloat Level** | **Action** | **Downtime** |
| :-- | :-- | :-- |
| < 5% | No action needed | None |
| 5-10% | Run `VACUUM ANALYZE` | None |
| 10-20% | Run `VACUUM FULL` during low-traffic | 30-60 seconds |
| > 20% | Run during maintenance window | 5-30 minutes |

```sql
-- For moderate bloat (5-20%)
VACUUM FULL ANALYZE reports_pending;  -- Rebuilds table, locks it during operation

-- For severe bloat (> 20%, use during maintenance window only)
BEGIN;
ALTER TABLE drainers RENAME TO drainers_old;
CREATE TABLE drainers AS SELECT * FROM drainers_old;
ALTER TABLE drainers ADD PRIMARY KEY (id);
-- Recreate all indexes...
DROP TABLE drainers_old;
COMMIT;
```


### Regular Maintenance Schedule[^32][^33]

For production, implement automated maintenance:

```sql
-- Using pg_cron extension (free, requires PostgreSQL 10+)
CREATE EXTENSION IF NOT EXISTS pg_cron;

-- Every 6 hours: Analyze high-write tables
SELECT cron.schedule('analyze-pending', '0 */6 * * *', 
  'ANALYZE reports_pending;'
);

-- Weekly: Full vacuum on high-write tables
SELECT cron.schedule('vacuum-reports', '0 2 * * 0',
  'VACUUM FULL ANALYZE reports_pending;'
);

-- Monthly: Reindex bloated indexes
SELECT cron.schedule('reindex-maintenance', '0 4 * * 1',
  'REINDEX INDEX CONCURRENTLY idx_drainers_wallet_active;'
);
```


***

## 6. Scaling Roadmap (100 to 10,000+ Daily Users)

### Stage 1: 100-500 Daily Users (Current - Single Instance)

**Database Setup:**

- Single PostgreSQL instance (8GB RAM, 4 CPU)
- Connection pool: 15-20 connections
- Estimated throughput: 50-100 queries/second

**Implementation:**

```sql
-- Week 1: Deploy indexes
CREATE INDEX idx_drainers_wallet_active ON drainers(wallet_address) WHERE status = 'active';
CREATE INDEX idx_analysis_results_gin ON analysis_cache USING GIN (analysis_results);

-- Week 2: Configure per-table autovacuum
ALTER TABLE reports_pending SET (autovacuum_vacuum_scale_factor = 0.05);

-- Week 3: Set up monitoring
CREATE EXTENSION pg_stat_statements;
```

**Cost:** \$50-100/month (free tier or shared host)

### Stage 2: 500-2,000 Daily Users (Introduce Read Scaling)

**Architecture Changes:**

- Primary database + 1-2 read replicas[^12][^13]
- Redis cache for hot queries[^35]
- Connection pool per replica: 10-15

**Implementation:**

```sql
-- On PRIMARY: Enable replication
ALTER SYSTEM SET wal_level = replica;
ALTER SYSTEM SET max_wal_senders = 10;
SELECT pg_reload_conf();

-- Create replication user
CREATE ROLE replication_user LOGIN REPLICATION PASSWORD 'secure_pwd';

-- On REPLICA: Stream changes via WAL
-- pg_basebackup -h primary.db -D /data -U replication_user -v -P -W
-- touch /data/standby.signal && pg_ctl start
```

**Query Routing:**

```javascript
// Route writes to primary, reads to replicas
const writeQuery = (sql, params) => primaryPool.query(sql, params);
const readQuery = (sql, params) => {
  const replicaIndex = Math.floor(Math.random() * replicas.length);
  return replicas[replicaIndex].query(sql, params);
};
```

**Cost:** \$300-500/month (primary + 1 replica)

### Stage 3: 2,000-10,000 Daily Users (Vertical Scaling + Analysis DB)

**Architecture Changes:**

- Upgrade primary to 32GB RAM, 16 CPU
- 2-3 read replicas for load distribution
- Separate analytical database (materialized views)[^36][^35][^37]
- Connection pool: 25-40

**Implementation:**

```sql
-- Create materialized view for dashboard analytics
CREATE MATERIALIZED VIEW drainer_statistics AS
SELECT 
  DATE_TRUNC('day', created_at) as report_date,
  attack_type,
  COUNT(*) as count,
  COUNT(DISTINCT user_id) as reporters
FROM reports_pending
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY DATE_TRUNC('day', created_at), attack_type;

-- Refresh every 6 hours
SELECT cron.schedule('refresh-stats', '0 */6 * * *',
  'REFRESH MATERIALIZED VIEW CONCURRENTLY drainer_statistics;'
);
```

**Cost:** \$1,200-1,500/month (1 large instance + 2 replicas)

### Stage 4: 10,000+ Daily Users (Horizontal Scaling with Citus)

Citus is a PostgreSQL extension that distributes data across multiple nodes, enabling horizontal scaling:[^14][^15]

**Architecture:**

- Coordinator node (query planning)
- 3+ worker nodes (data storage + execution)
- Sharding by `wallet_address` or `user_id`

**Implementation:**

```sql
-- Install Citus
CREATE EXTENSION citus;

-- Add worker nodes
SELECT * FROM citus_add_node('worker1.example.com', 5432);
SELECT * FROM citus_add_node('worker2.example.com', 5432);
SELECT * FROM citus_add_node('worker3.example.com', 5432);

-- Shard tables by wallet_address
SELECT create_distributed_table('drainers', 'wallet_address');
SELECT create_distributed_table('reports_pending', 'wallet_address');
SELECT create_distributed_table('analysis_cache', 'wallet_address');

-- Queries now parallelize automatically
SELECT attack_type, COUNT(*) as count
FROM reports_pending
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY attack_type;  -- Executes in parallel across all workers
```

**Benefits:**

- 10-100x throughput increase through parallelism
- Linear cost scaling (add nodes as needed)
- Maintains PostgreSQL compatibility

**Cost:** \$2,000+/month (3-4 servers + Citus Enterprise)

***

## Implementation Recommendations for Hackathon (14-Day Deadline)

### Week 1 (Days 1-7): Foundation

- ✅ Day 1-2: Deploy indexes (20 minutes) + connection pooling (1 hour)
- ✅ Day 3-4: Enable `pg_stat_statements` and set up monitoring dashboard
- ✅ Day 5-6: Run EXPLAIN ANALYZE on all critical queries
- ✅ Day 7: Fix N+1 queries and SELECT * anti-patterns


### Week 2 (Days 8-14): Optimization \& Testing

- ✅ Day 8-9: Tune autovacuum settings for high-write tables
- ✅ Day 10: Load test with 10x expected concurrent users
- ✅ Day 11-12: Implement caching for analysis_cache JSONB queries
- ✅ Day 13: Set up monitoring alerts (pool saturation > 80%, slow queries > 200ms)
- ✅ Day 14: Documentation and operational handoff


### Performance Targets

- Query latency: < 100ms (P95)
- Cache hit ratio: > 95%
- Pool saturation: < 60%
- Table bloat: < 5%

***

## Best Practices Summary

1. **Indexing:** Always include partial indexes on frequently filtered columns (status, active_flag)[^16][^17]
2. **JSONB:** Normalize hot-path columns to regular columns; use GIN for rare complex queries[^2][^10]
3. **Connection Pooling:** Size pool at (CPU cores * 2) + 5; monitor saturation[^23]
4. **Queries:** Use EXPLAIN ANALYZE for all queries > 50ms; eliminate N+1 patterns[^18][^28][^29]
5. **Maintenance:** Run monthly bloat check; tune autovacuum per table[^20][^21]
6. **Monitoring:** Capture baseline metrics weekly; alert on 20% degradation[^8]

***

## Cost Considerations (Always Prefer Free/Generous Services)

**Recommended for Hackathon:**

- **Neon** (free tier + connection pooling)
- **Supabase** (free tier + real-time for wallet events)
- **Railway** (\$5/month with free credits)

**Production Options:**

- Single instance: \$50-100/month
- With read replica: \$300-500/month
- Distributed (Citus): \$2,000+/month

***

## Key Resources Provided

**Comprehensive Optimization Guide** - Complete reference with schema designs, monitoring queries, and maintenance procedures

**Implementation Code (TypeScript)** - Production-ready connection pooling, circuit breaker, query executor, and monitoring middleware

**SQL Monitoring Queries** - Ready-to-use diagnostics for bloat detection, slow query analysis, and performance trending

**Architecture Decision Tree** - Visual flowchart showing optimization decision paths

![PostgreSQL Performance Optimization Decision Tree for Have I Been Drained](https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/60f2fa59c5effa2b31e19e18058162d5/0e1bc94e-64da-42a9-ae1f-0e8809fac236/04220b8c.png)

PostgreSQL Performance Optimization Decision Tree for Have I Been Drained

All resources are specific to the "Have I Been Drained" project schema (drainers, analysis_cache, reports_pending) and focus on real-world performance gains measurable during the 14-day hackathon window.
<span style="display:none">[^38][^39][^40][^41][^42][^43][^44][^45][^46][^47][^48][^49][^50]</span>

<div align="center">⁂</div>

[^1]: https://www.mydbops.com/blog/postgresql-indexing-best-practices-guide

[^2]: https://www.crunchydata.com/blog/indexing-jsonb-in-postgres

[^3]: https://neon.com/docs/connect/connection-pooling

[^4]: https://www.postgresql.org/docs/current/indexes-types.html

[^5]: https://neon.com/postgresql/postgresql-indexes/postgresql-index-types

[^6]: https://cubeapm.com/blog/best-postgresql-monitoring-tools/

[^7]: https://neon.com/blog/postgres-support-recap-investigating-postgres-query-performance

[^8]: https://last9.io/blog/postgresql-performance/

[^9]: https://www.architecture-weekly.com/p/postgresql-jsonb-powerful-storage

[^10]: https://aws.amazon.com/blogs/database/postgresql-as-a-json-database-advanced-patterns-and-best-practices/

[^11]: https://leapcell.io/blog/choosing-your-postgresql-connection-proxy

[^12]: https://supabase.com/docs/guides/platform/read-replicas

[^13]: https://www.percona.com/blog/guide-to-postgresql-replication-with-both-asynchronous-and-synchronous-standbys/

[^14]: https://openmetal.io/resources/blog/powering-your-data-warehouse-with-postgresql-and-citus-on-openmetal-for-distributed-sql-at-scale/

[^15]: https://www.citusdata.com

[^16]: https://www.heap.io/blog/speeding-up-postgresql-queries-with-partial-indexes

[^17]: https://www.postgresql.org/docs/current/indexes-partial.html

[^18]: https://talent500.com/blog/detect-optimize-slow-postgresql-queries/

[^19]: https://www.enterprisedb.com/blog/postgresql-query-optimization-performance-tuning-with-explain-analyze

[^20]: https://www.tigerdata.com/learn/how-to-reduce-bloat-in-large-postgresql-tables

[^21]: https://dbsnoop.com/how-to-remove-bloat-in-postgresql-tables-indexes/

[^22]: https://stackoverflow.com/questions/58804517/postgresql-connection-pooling-in-a-nodejs-cluster

[^23]: https://node-postgres.com/features/pooling

[^24]: https://scalegrid.io/blog/how-to-connect-postgresql-database-in-node-js/

[^25]: https://pganalyze.com/webinars/how-to-optimize-slow-queries-with-EXPLAIN

[^26]: https://www.tigerdata.com/learn/strategies-for-improving-postgres-join-performance

[^27]: https://medium.datadriveninvestor.com/sql-query-optimization-in-2025-7-simple-techniques-for-faster-database-performance-cf6ec06596d0

[^28]: https://www.bigdba.com/postgresql/2261/postgresql-red-flags-expert-guide-to-database-anti-patterns-essential-extensions/

[^29]: https://stackoverflow.com/questions/97197/what-is-the-n1-selects-problem-in-orm-object-relational-mapping

[^30]: https://planetscale.com/blog/what-is-n-1-query-problem-and-how-to-solve-it

[^31]: https://www.cybertec-postgresql.com/en/join-strategies-and-performance-in-postgresql/

[^32]: https://www.mydbops.com/blog/postgresql-parameter-tuning-best-practices

[^33]: https://www.cybrosys.com/research-and-development/postgres/how-to-tune-autovacuum-in-postgresql-for-fast-moving-performance

[^34]: https://haiderzdbre.substack.com/p/postgresql-table-bloat-diagnostic-sql-queries

[^35]: https://neon.com/guides/caching-with-materialized-views

[^36]: https://risingwave.com/blog/mastering-materialized-views-in-postgresql/

[^37]: https://dohost.us/index.php/2025/11/24/implementing-materialized-views-postgresql-or-cached-tables-mysql-for-complex-reports/

[^38]: https://www.tigerdata.com/learn/postgresql-performance-tuning-optimizing-database-indexes

[^39]: https://www.reddit.com/r/PostgreSQL/comments/1k2a7v2/postgres_using_index_with_filter_condition/

[^40]: https://www.tigerdata.com/blog/best-practices-for-query-optimization-in-postgresql

[^41]: https://docs.cloud.google.com/sql/docs/postgres/replication

[^42]: https://www.pgedge.com/blog/scaling-postgres

[^43]: https://www.tigerdata.com/learn/best-practices-for-postgres-database-replication

[^44]: https://www.clever.cloud/developers/guides/pgpool/

[^45]: https://www.netdata.cloud/blog/postgresql-database-bloat/

[^46]: https://www.instaclustr.com/education/postgresql/postgresql-tuning-6-things-you-can-do-to-improve-db-performance/

[^47]: https://www.datacamp.com/doc/postgresql/expression-indexes

[^48]: https://neon.com/postgresql/postgresql-indexes/postgresql-index-on-expression

[^49]: https://www.geeksforgeeks.org/postgresql/postgresql-index-on-expression/

[^50]: https://www.cybrosys.com/research-and-development/postgres/how-functional-indexes-improve-query-performance-in-postgresql

